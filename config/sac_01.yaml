policy: "MlpPolicy"
learning_rate: 5e-4   # Slightly higher learning rate
gamma: 0.98           # A bit lower gamma for faster learning
tau: 0.005            # Keep tau the same for target smoothing
batch_size: 512       # Larger batch size for more stable updates
buffer_size: 1000000  # Standard buffer size
train_freq: 2         # Train less frequently to balance updates
gradient_steps: 2     # More gradient steps per update
action_noise: 0.2     # Increased action noise for exploration
total_timesteps: 200_000  # Longer training for better convergence
batch_size: 128       # Adjust batch size to a more balanced value
seed: 42              # Change random seed
eval_episodes: 1000   # Same evaluation setup
max_steps: 250        # Allow more steps per episode
